\documentclass[11pt]{report}

\usepackage[utf8]{inputenc}
\usepackage[letterpaper,bindingoffset=0.2in,%
            left=1in,right=1in,top=1in,bottom=1in,%
            footskip=.25in]{geometry}
\usepackage{microtype}
%\usepackage[T1]{fontenc}
%\usepackage{newpxtext,newpxmath}
\usepackage{mathtools}
%\usepackage{amsthm}
\usepackage{latexsym, mathrsfs}
%\usepackage[amsmath]{ntheorem}
\usepackage{graphicx}
\usepackage{subcaption}
%\usepackage[scaled]{beramono}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage[linesnumbered,ruled,procnumbered]{algorithm2e}
\usepackage{enumitem}
\usepackage{multirow}
%\usepackage{wrapfig}
\usepackage{xspace}
\usepackage{textcomp}
\usepackage{listings}
% \usepackage{mathdots}
\usepackage{setspace}
\usepackage{tikz}
\usepackage{hhline}
\usepackage[numbers,sort]{natbib}
\usepackage[nottoc]{tocbibind}
\usepackage[normalem]{ulem}
\usepackage[titletoc]{appendix}
%\usepackage{subfig}
%\usepackage{subfloat}

\usepackage{titlesec}
\titleformat{\chapter}
  {\normalfont\LARGE\bfseries}{\thechapter}{1em}{}
\titlespacing*{\chapter}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}

% VQuel listing
\newcommand{\mysinglespacing}{%
  \setstretch{1}% no correction afterwards
}
\lstset{basicstyle=\footnotesize\ttfamily\mysinglespacing,breaklines=true,showstringspaces=false,
numbers=left,numbersep=5pt,
language=Java,escapeinside={(*@}{@*)}}
\lstset{emph={
    range, of, is, retrieve, into, where, all, pk, sort, by, and, or, not, count, count_all,
    avg, sum, min, max, avg_all, sum_all, min_all, max_all, abs, unique, any, group, as
    },emphstyle={\bfseries}
}
\lstset{deletestring=[b]{"}}
\lstMakeShortInline[columns=fixed]!


\usepackage[pdfpagelabels=false]{hyperref}
\hypersetup{
   colorlinks,
   linkcolor={red!50!black},
   citecolor={blue!50!black},
   urlcolor={blue!80!black}
}

\SetAlFnt{\small}

\mdfdefinestyle{MyFrame}{%
    linecolor=black,
    outerlinewidth=1pt,
    roundcorner=10pt,
    innertopmargin=\baselineskip,
    innerbottommargin=\baselineskip,
    innerrightmargin=20pt,
    innerleftmargin=20pt,
    backgroundcolor=gray!50!white}
    
\newenvironment{denselist}{
    \begin{list}{\small{$\bullet$}}%
    {\setlength{\itemsep}{0ex} \setlength{\topsep}{0ex}
    \setlength{\parsep}{0pt} \setlength{\itemindent}{0pt}
    \setlength{\leftmargin}{1.5em}
    \setlength{\partopsep}{0pt}}}%
    {\end{list}}

\usepackage{cleveref}
\crefname{section}{\S\!\!\!\;}{\S\S}
\Crefname{section}{\S}{\S\S}
\crefname{lstlisting}{listing}{listings}
\Crefname{lstlisting}{Listing}{Listings}

\numberwithin{equation}{section}
\newtheorem{theorem}{Theorem}
\newtheorem{proof}{Proof}
\newtheorem{query}{Query}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{formulation}{Problem}
\newtheorem{definition}{Definition}
\newtheorem{remark}[theorem]{Remark}


\newcommand{\topic}[1]{\vspace{-3.5pt}\smallskip \smallskip \noindent{\bf #1:}}
\newcommand{\stitle}[1]{\vspace{0.5em}\noindent\textbf{#1}}
\renewcommand{\stitle}[1]{\vspace{1em}\noindent\textbf{#1}}

\newcommand{\vv}{\mathcal{V}\xspace}
\newcommand{\gcal}{\mathcal{G}\xspace}
\newcommand{\ee}{\mathcal{E}\xspace}
\newcommand{\cc}{\mathcal{C}\xspace}
\newcommand{\rr}{\mathcal{R}\xspace}
\newcommand{\hh}{\mathcal{H}\xspace}
\newcommand{\pp}{\mathcal{P}\xspace}
\newcommand{\mm}{\mathcal{M}\xspace}

\newcommand{\kk}{\mathcal{K}\xspace}    % Universe of keys
\newcommand{\sg}{\mathcal{G}\xspace}    % Storage graph
\newcommand{\qt}{\mathcal{G}_Q\xspace}  % Access tree
\newcommand{\qq}{\mathcal{Q}\xspace}    % Query
\newcommand{\qa}{\mathcal{A}\xspace}    % Multi-object subset
\newcommand{\ck}{\textsc{Checkout}\xspace}

\newcommand{\D}[1]{\Delta_{#1}}
\newcommand{\Dp}[1]{\Delta_{#1}^{+}}
\newcommand{\Dm}[1]{\Delta_{#1}^{-}}

\newcommand{\dsvc}{\textsc{DEX}\xspace}
\newcommand{\dhub}{\textsc{DataHub}\xspace}
%\newcommand{\df}{\texttt{datafile}\xspace}
%\newcommand{\dfs}{\texttt{datafile}s\xspace}

\newcommand{\ds}{{\sc RStore}\xspace}
\newcommand{\bt}{{\sc Bottom-Up}\xspace}
\newcommand{\sh}{{\sc Shingle}\xspace}
\newcommand{\dfs}{{\sc DepthFirst}\xspace} 
\newcommand{\bfs}{{\sc BreadthFirst}\xspace}
\newcommand{\deltat}{{\sc Delta}\xspace}
\newcommand{\subc}{{\sc SubChunk}\xspace}



%COMPRESS
\newcommand{\squeezeup}{\vspace{-2.5mm}}

% Debugging
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}
\newcommand{\amol}[1]{\textcolor{red}{[Amol: #1]}}
\newcommand{\resolved}[1]{\textcolor{green}{[Amol: #1]}}

\newcommand{\eat}[1]{}
\newcommand{\todo}[1]{\textcolor{blue}{[TODO: #1]}}


\title{
{\bf Teaching Machines to Ask Clarification Questions}\\
\vspace{18pt}
\it Preliminary Oral Exam (Thesis Proposal)}

\author{
{\bf Sudha Rao}  \\
Department of Computer Science \\
University of Maryland, College Park\\
{\texttt{raosudha@cs.umd.edu}}
}


\date{
\vspace{42pt}
Dissertation Proposal submitted to: \\
Department of Computer Science \\
University of Maryland, College Park, MD 20742 \\
\bigskip
\bigskip
\today
\bigskip
\bigskip
\begin{table}[htp]
\begin{center}
\begin{tabular}{lll}
&\multicolumn{2}{l}{Advisory Committee:} \\ \\
Dr. Hal Daum\`{e} III & Chair & U. of Maryland, College Park \\
Dr. David Jacobs & Dept's Rep & U. of Maryland, College Park \\
Dr. Philip Resnik & Member & U. of Maryland, College Park \\
Dr. Lucy Vanderwende & Member & Microsoft Research \\
\end{tabular}
\end{center}
\end{table}%
}


\begin{document}

\pagestyle{plain}
\pagenumbering{roman}

\maketitle
\pagebreak

\begin{abstract}
\normalsize

An overarching goal of the natural language processing community is to develop techniques that would enable machines to process naturally occurring text as efficiently as humans do. However, as humans, we do not always understand each other. According to Gricean pragmatics \cite{grice1975logic}, speakers and listeners adhere to a Cooperative Principle where a speaker communicates information that is as informative as required and not more. In case of a knowledge gap, the listener resorts to asking questions. With the advancements of artificial intelligence technologies, we increasingly find ourselves interacting with automated agents (like Apple's Siri, Amazon's Alexa, Google Home, etc) in naturally spoken languages. If we wish to make such human-bot interactions as efficient as human-human interactions are, it is important that we teach machines to ask clarification questions when faced with uncertainty or knowledge gaps.

In the field of natural language processing, however, despite decades of work on question-answering, there has been little work in question asking. Moreover, most of the previous work has focused on generating reading comprehension style questions which, by definition, are answerable from the provided text. The goal of my dissertation work is to teach machines to ask clarification questions i.e. questions that point at the missing information in a text. Primarily, we focus on two scenarios where we find such question asking to be useful: (1) clarification questions on posts found in community-driven Q\&A forums like StackExchange (2) clarification questions during goal-oriented dialogue sessions. 

In our first line of research, we study the problem of question generation using data from StackExchange, a plentiful online resource in which people routinely ask clarifying questions to posts so that they can better offer assistance to the original poster. We create a novel dataset using StackExchange and in our preliminary work, we address the question selection problem i.e. select the right clarification question from a set of prior questions. We develop a novel neural-network model inspired by the notion of expected value of perfect information: a good question is one whose expected answer is going to be most useful. To build more generalizable systems, in my first proposed work, I propose the following two strategies: (a) a template based question generation model and (b) an encoder-decoder based neural generative model.

In our second line of research, we study the problem of question generation using the Ubuntu Dialogue Corpus, a two-way conversational data extracted systematically from chat logs where people discuss issues with their Ubuntu Operating system. In our preliminary work, we build a neural network model for the task of predicting the next best response given a context of a conversation using best practices in dialogue modeling coupled with our novel vocabulary selection strategies. In my second proposed work, I plan to build a novel strategy for generating a clarification question as the next response given different levels of context of a conversation. 

In both the research ideas described so far, we learn to generate clarification questions statistically i.e. by looking at previously asked questions in a similar context. However as humans we infer a knowledge gap only when we understand what information completes a given use-case. Therefore, in my third proposed work, I plan to make use of external knowledge sources to understand what is missing in a given context and then ask a clarification question.  

\end{abstract}

\pagebreak


\tableofcontents
\pagebreak

\cleardoublepage
\pagenumbering{arabic}


\chapter{Introduction}

\section{Motivation}

Asking questions is fundamental to communication, and machines cannot effectively collaborate with humans unless they can ask questions. Asking questions is also a natural way for machines to express uncertainty, a task of increasing importance in an automated society. Despite decades of work on question answering, there is relatively little work in question asking.

An overarching goal of the natural language processing community is to develop techniques that would enable machines to process naturally occurring text as efficiently as humans do. However, as humans, we do not always understand each other. According to Gricean pragmatics \cite{grice1975logic}, speakers and listeners adhere to a Cooperative Principle where a speaker communicates information that is as informative as required and not more. In doing so, the speaker assumes a certain common ground with the listener. In case of a knowledge gap, the listener resorts to asking questions. With human-bot interaction increasingly becoming common place, it is important we teach machines how to ask clarification questions so that it can attain a mutual understanding with the human. 

The extent of informativeness depends on the speaker's understanding of what she thinks is the common ground between the speaker and the listener. Communication is still possible because the listener makes use of this powerful tool of asking questions. By asking questions the listener can elicit information that she thinks is unclear or missing and thus attain a mutual understanding with the speaker. \cite{graesser2008question} identify that one of the key purposes of asking questions is the correction of such knowledge deficits.\\

\noindent
In the field of natural language processing, however, despite decades of work on question answering, there has been little work in question asking. Moreover most of the previous work on generating questions has been on generating reading comprehension style questions: given a text, write a question that one might find on a standardized test assessing the knowledge of a student about a particular topic in the text. For a machine to effectively collaborate with humans, it is important that it learns to ask a question when faced with uncertainty, a task of increasing importance in an automated society. For example, consider a scenario where an automated agent is trying to assist you in solving a problem. You will describe your problem to the agent. The agent on the other end has a certain understanding of what information is required to solve that problem. The agent can either make an assumption and work under uncertainty OR ask a question. The goal of this thesis work is to explore how can a machine automatically generate clarification questions when faced with such uncertainty or knowledge gaps

\section{Research questions}

About a decade ago, if you faced a technical problem the only way to solve it would be to go to an expert. Due to the recent surge in the use of internet, a lot of such problem solving happens online these days on question answering (Q\&A) forums where users post their problems and other users reply to them providing assistance. However \cite{asaduzzaman2013answering} observed that on StackExchange, one such community-driven problem solving platforms, many a times the posts go unanswered for a long time because they are not clear enough i.e. they are missing some information. Other users then ask clarification questions to those posts so that they can better offer assistance to the original poster. Our first research question is the following: \textit{Can we build a model that can learn to automatically generate a clarification question to a post by looking at clarification questions posted in the forum before?}\\

\noindent
Human-bot interaction has become increasingly popular in recent times. Apple's siri, Amazon's alexa, IBM's watson are all examples of successful advancements in the Artificial Intelligence technology. These technologies are great when it comes to simple natural language based interactions like ``What is the weather like in New York City?'' OR ``Play me latest top 10 songs'', etc. However when one tries to use these interfaces for more complicated purposes like ``I can't start my laptop. Help me\!'', OR ``Find me a recipe for lasagne'', within a few interactions one would soon give up. One key reasons for such failures is the lack of common understanding between the human user and the bot. The human user has a certain understanding of the problem/request he has and many a times he fails to convey the same understanding to the bot. In such a scenario, the bot can be much more useful if it could try to establish this common understanding by asking relevant questions. Our second research question is: \textit{Can we build an interactive model that can learn to ask clarification questions when faced with uncertainty or a knowledge gap?}\\

\noindent
% clarification question with the help of knowledge base i.e. find out what is missing and then ask a question

\section{Proposed solutions}

In order to learn how to ask clarification questions, we build a model inspired by the decision theoretic framework of expected value of perfect information (EVPI). EVPI is a measurement of of the value of gathering information. A good question is the one whose likely answer is going to be the most useful. In our setting, we use EVPI to calculate which question is most likely to elicit an answer that would make the post more informative. On StackExchange, users routinely ask clarification questions to post. The author of the post subsequently edits the post answering the question. We mine many (post, question, answer) triples using StackExchange's edit histories. We extract the initial post as p, question posted in the comments section as q, and edit to the original post as answer a to form our (p,q,a) triples. Using this data, we build our EVPI inspired neural network model which learns to ask a clarification question given a post.\\

\noindent
In our preliminary work, we focus on the question selection problem i.e. select the right clarification question from a set of prior questions. Given a post, we first generate a set of candidate questions by identifying posts similar to the given post and then looking at the questions asked to those posts. For identifying similar posts, we use Lucene \footnote{https://lucene.apache.org/}, a software extensively used in information retrieval for extracting documents relevant to a given query from a pool of documents. To enable our system to generalize better on new unseen cases, we propose two research directions: a template based question generation method where we first generate templates and then fill in the variables using the current context of the post; and a sequence-to-sequence based neural generative model where we generate the question one word at a time given a post.\\

\noindent
We study the problem of generation clarification questions in a dialog setting using the Ubuntu Dialog corpus, a large corpus recently put together by \cite{} by extracting two-way conversations from chat logs where people were discussing about the issues they were having with the Ubuntu Operating system. In our preliminary work, we consider the problem of next utterance classification i.e. given a context of a conversation and a set of possible next responses, choose the correct next response. %Recently, \cite{serban2016multiresolution} proposed a multi-resolution recurrent neural network model where they generate the high-level coarse tokens like nouns, entities and activities using a separate layer in their model thus indirectly giving more importance to the topic words in a conversation. Our model, on the other hand, that captures a similar notion using what we call the `bursty vocabulary' i.e. we up-weight the words that occur frequently within the short context of a conversation, thus avoiding the need for manually crafting the list of coarse tokens. 
Our model combines several best practices in dialogue modeling such as utterance level hierarchy, attention, character trigram histograms and context based vocabulary selection strategy. 
We build a novel neural network based model using what we call the `bursty vocabulary' i.e. up-weighting the words that occur frequently within the short context of a conversation, thus indirectly giving more importance to topic words. This approach gives us significant improvements over current neural network baselines. In our next step, we propose to look at the responses in the conversations that are questions and build a model that learns to generate a question based response given different levels of context of a conversation.

%this para -- we also explore question generation in dialogue setting. work in dialogue -- next utterance classification -- moving on to generating questions in mult-turn dialogue -- generative models on ubuntu dialogue dataset

\newpage

\chapter{Related work}

\section{Question Generation}

The problem of question generation has received sparse attention from the natural language processing community. Most prior work focuses on generating reading comprehension questions:  given text, write questions that one might find on a standardized test \cite{vanderwende2008importance,heilman2011automatic,rus2011question,olney2012question}.  Comprehension questions, by definition, are answerable from the provided text. Clarification questions are not.  

Outside reading comprehension questions, \cite{labutov2015deep} generate high-level question templates by crowdsourcing and given a text segment, rank question templates that are relevant. However the crowdsourcing method of collecting data leads to significantly less data than we collect using our method. \cite{liu2010automatic} use template question generation to help authors write better related work sections. \cite{mostafazadeh2016generating} introduce a Visual Question Generation task where they consider question generation from images, a multi-modal variant of question generation. 
\cite{penas2010filling} identify the notion of missing information similar to us but they attempt to fill the knowledge gaps in a text with the help of external knowledge bases, whereas we instead ask clarification questions. \cite{artzi2011bootstrapping} use human-generated clarification questions to drive a semantic parser where the clarification questions are aimed towards simplifying a user query; whereas we generate clarification questions aimed at  identifying missing information in a text. 

- Question generation shared task: http://www.aclweb.org/anthology/W11-2853

\section{Neural networks in NLP}

\section{Dialogue modeling}

Following the introduction of the Ubuntu dialogue dataset, there has been several recent work in modeling dialogue that uses this dataset. \cite{DBLP:conf/sigdial/LowePSP15} describe two neural network baseline models using Recurrent Neural Network (RNN) and Long Short Term Memory (LSTM). 
\cite{DBLP:journals/corr/KadlecSK15} create an ensemble of LSTMs, Bi-LSTMs and CNNs to get improved baselines. 
\cite{xu2016incorporating} incorporate domain knowledge by enhancing their LSTM model with a recall gate.
\cite{serban2016hierarchical} introduce a latent variable hierarchical recurrent encoder-decoder model by adding a context level RNN that processes sequences of sub-sequences operating on top of the token level RNN.
\cite{serban2016multiresolution} take a similar hierarchical approach but instead of defining a latent coarse representation, they assume it to be observed and experiment with two such representations (sequence of nouns and sequence of activities and entities).
\cite{baudivs2016sentence} introduce a unified approach for scoring sentence pairs applicable to number of tasks including next utterance classification and show best result on the Ubuntu dialogue corpus task. 

The techniques we combine in this work are best practices gleaned from the NLP modeling literature. 
The character trigram histogram technique (Section~\ref{character_level_modeling}) was introduced as word hashing \cite{DBLP:conf/cikm/HuangHGDAH13} to reduce the dimensionality of bag-of-words term vectors. Residual learning (Section~\ref{residual_learning}) was first introduced as a way to ease training of very deep networks in image recognition work \cite{he2015deep}. The technique of attention aggregation (Section~\ref{attention_aggregation} has been effectively used for several language based tasks like neural machine translation \cite{DBLP:journals/corr/BahdanauCB14}, \cite{DBLP:conf/emnlp/LuongPM15}, question answering \cite{} and others. More specifically in dialogue modeling, \cite{DBLP:journals/corr/YaoPZW16} have shown that attending to the context while generating the words of the response, similar to attention based model used for MT, gives an improvement over using just the last hidden state.


\newpage

\chapter{Generating clarification questions on StackExchange posts}

\newpage

\chapter{Generating next response during two-way conversation}

\newpage

\chapter{Proposed Work}

\section{Template based question generation}

In our preliminary work, we focused on the question selection problem i.e. select the right clarification question from a set of prior questions. To make the system more general, it needs to be able to generalize to unseen context well. For e.g. consider a scenario in which the post is talking about the application `apt-get' that the model has never seen in its past before. However, from the context, it can make out that the discussion is similar to a different discussion where the topic was the application `yum' and a plausible clarification question is asking for the version of the application. However, since it has not seen a clarification question like ``What version of yum are you using?'', it fails to generate one. To circumvent this generalization issue, we propose template based question generation method. For instance, consider a template like ``What version of \_\_\_  are you running?''. This template can generate thousands of specific variants found in the data like ``What version of Ubuntu are you running?'', ``What version of OS are you running?'', ``What version of apt-get are you running?'', etc. We enumerate four steps of our template based question generation method:
\begin{enumerate}
\item Identify groups of similar questions from a huge collection of questions.
\item Generate a template for each group of questions.
\item Given a post, select a question template from a set of candidate question templates.
\item Fill in the variables in the template using words relevant to the given post.
\end{enumerate}

For grouping similar questions together, we plan to explore some of the clustering algorithms which cluster documents based on their lexical and semantic similarity. A preliminary experiment of trying to cluster questions has revealed that the questions are too different from each other to form useful clusters. We therefore propose a different strategy in which we will remove topic specific words from the questions and then cluster them, thus forcing a generalization before grouping them. To generate templates corresponding to each cluster, we plan to further identify topic specific words from the question using syntactic information (like parts-of-speech tags) and then remove them to form generic templates. In the third step, given a post and a set of candidate question templates, we will select a template using a model similar to our preliminary work. Finally, in the fourth step, we plan to explore methods similar to slot filling approaches \cite{} to populate the blanks in the template with post specific words.

\newpage
\section{Sequence-to-sequence based question generation}

Sequence-to-sequence neural network models have proven to be effective for several language generation tasks like machine translation (Sutskever et.al. 2014), dialog generation (Serban, Iulian V., et al. 2016), etc. The main idea behind these models is that they work on an encoder-decoder framework. The encoder model takes in the words of the input one word at a time, passes it through a recurrent neural network (or its variants like LSTM, GRU, etc) and generates a final vector representation of the input. The decoder, then, takes in this input representation and a start symbol and generates the words of the output one word at a time till it generates the end of sequence symbol. The model is trained using a large number of input/output sequences to minimize the perplexity between the generated output sequence and the true output sequence.\\

\noindent
On similar lines, we propose a model for generating the clarification question one word at a time, given the words of a post. A recent neural generative question answering model (Yin, Jun, et al.  2016) built an answer language model which decides, at each time step, whether to generate a common vocabulary word or an answer word retrieved from a knowledge base. Thus the model is built on an encoder-decoder framework equipped with the ability to enquire a knowledge base of facts contained in the form of entity-relationship triples. Inspired from this work, we propose to build a question generation model which will decide, at each time step, whether to generate a common vocabulary word or a topic specific word retrieved from the current post. Using this approach we thus incorporate the template based question generation method into a more general neural network framework.

\section{Clarification questions for dialog systems}

In our preliminary work we developed a model for generating a clarification question in a non-interactive setting i.e. after someone has described their problem in its entirety, we ask a question. However, most of such problem solving happens in the form of a dialog between a system and a human. In our next research direction, we propose to build a model that learns to generate  clarification questions at different stages of an ongoing conversation. Our model will learn the trade-off between asking a generic question at the initial stages and asking a specific question at the later stages of a conversation. We will evaluate our model using the Ubuntu Dialogue corpus, a large corpus recently put together by Lowe, Ryan, et al. (2015) by extracting two-way conversations from chat logs where people were discussing about the issues they were having with the Ubuntu Operating system.

\newpage

\begin{small}
\bibliographystyle{acl2017}
\bibliography{proposal}
\end{small}

\end{document}


