\documentclass[11pt]{report}

\usepackage[utf8]{inputenc}
\usepackage[letterpaper,bindingoffset=0.2in,%
            left=1in,right=1in,top=1in,bottom=1in,%
            footskip=.25in]{geometry}
\usepackage{microtype}
%\usepackage[T1]{fontenc}
%\usepackage{newpxtext,newpxmath}
\usepackage{mathtools}
%\usepackage{amsthm}
\usepackage{latexsym, mathrsfs}
%\usepackage[amsmath]{ntheorem}
\usepackage{graphicx}
\usepackage{subcaption}
%\usepackage[scaled]{beramono}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage[linesnumbered,ruled,procnumbered]{algorithm2e}
\usepackage{enumitem}
\usepackage{multirow}
%\usepackage{wrapfig}
\usepackage{xspace}
\usepackage{textcomp}
\usepackage{listings}
% \usepackage{mathdots}
\usepackage{setspace}
\usepackage{tikz}
\usepackage{hhline}
\usepackage[numbers,sort]{natbib}
\usepackage[nottoc]{tocbibind}
\usepackage[normalem]{ulem}
\usepackage[titletoc]{appendix}
%\usepackage{subfig}
%\usepackage{subfloat}

\usepackage{titlesec}
\titleformat{\chapter}
  {\normalfont\LARGE\bfseries}{\thechapter}{1em}{}
\titlespacing*{\chapter}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}

% VQuel listing
\newcommand{\mysinglespacing}{%
  \setstretch{1}% no correction afterwards
}
\lstset{basicstyle=\footnotesize\ttfamily\mysinglespacing,breaklines=true,showstringspaces=false,
numbers=left,numbersep=5pt,
language=Java,escapeinside={(*@}{@*)}}
\lstset{emph={
    range, of, is, retrieve, into, where, all, pk, sort, by, and, or, not, count, count_all,
    avg, sum, min, max, avg_all, sum_all, min_all, max_all, abs, unique, any, group, as
    },emphstyle={\bfseries}
}
\lstset{deletestring=[b]{"}}
\lstMakeShortInline[columns=fixed]!


\usepackage[pdfpagelabels=false]{hyperref}
\hypersetup{
   colorlinks,
   linkcolor={red!50!black},
   citecolor={blue!50!black},
   urlcolor={blue!80!black}
}

\SetAlFnt{\small}

\mdfdefinestyle{MyFrame}{%
    linecolor=black,
    outerlinewidth=1pt,
    roundcorner=10pt,
    innertopmargin=\baselineskip,
    innerbottommargin=\baselineskip,
    innerrightmargin=20pt,
    innerleftmargin=20pt,
    backgroundcolor=gray!50!white}
    
\newenvironment{denselist}{
    \begin{list}{\small{$\bullet$}}%
    {\setlength{\itemsep}{0ex} \setlength{\topsep}{0ex}
    \setlength{\parsep}{0pt} \setlength{\itemindent}{0pt}
    \setlength{\leftmargin}{1.5em}
    \setlength{\partopsep}{0pt}}}%
    {\end{list}}

\usepackage{cleveref}
\crefname{section}{\S\!\!\!\;}{\S\S}
\Crefname{section}{\S}{\S\S}
\crefname{lstlisting}{listing}{listings}
\Crefname{lstlisting}{Listing}{Listings}

\numberwithin{equation}{section}
\newtheorem{theorem}{Theorem}
\newtheorem{proof}{Proof}
\newtheorem{query}{Query}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{formulation}{Problem}
\newtheorem{definition}{Definition}
\newtheorem{remark}[theorem]{Remark}


\newcommand{\topic}[1]{\vspace{-3.5pt}\smallskip \smallskip \noindent{\bf #1:}}
\newcommand{\stitle}[1]{\vspace{0.5em}\noindent\textbf{#1}}
\renewcommand{\stitle}[1]{\vspace{1em}\noindent\textbf{#1}}

\newcommand{\vv}{\mathcal{V}\xspace}
\newcommand{\gcal}{\mathcal{G}\xspace}
\newcommand{\ee}{\mathcal{E}\xspace}
\newcommand{\cc}{\mathcal{C}\xspace}
\newcommand{\rr}{\mathcal{R}\xspace}
\newcommand{\hh}{\mathcal{H}\xspace}
\newcommand{\pp}{\mathcal{P}\xspace}
\newcommand{\mm}{\mathcal{M}\xspace}

\newcommand{\kk}{\mathcal{K}\xspace}    % Universe of keys
\newcommand{\sg}{\mathcal{G}\xspace}    % Storage graph
\newcommand{\qt}{\mathcal{G}_Q\xspace}  % Access tree
\newcommand{\qq}{\mathcal{Q}\xspace}    % Query
\newcommand{\qa}{\mathcal{A}\xspace}    % Multi-object subset
\newcommand{\ck}{\textsc{Checkout}\xspace}

\newcommand{\D}[1]{\Delta_{#1}}
\newcommand{\Dp}[1]{\Delta_{#1}^{+}}
\newcommand{\Dm}[1]{\Delta_{#1}^{-}}

\newcommand{\dsvc}{\textsc{DEX}\xspace}
\newcommand{\dhub}{\textsc{DataHub}\xspace}
%\newcommand{\df}{\texttt{datafile}\xspace}
%\newcommand{\dfs}{\texttt{datafile}s\xspace}

\newcommand{\ds}{{\sc RStore}\xspace}
\newcommand{\bt}{{\sc Bottom-Up}\xspace}
\newcommand{\sh}{{\sc Shingle}\xspace}
\newcommand{\dfs}{{\sc DepthFirst}\xspace} 
\newcommand{\bfs}{{\sc BreadthFirst}\xspace}
\newcommand{\deltat}{{\sc Delta}\xspace}
\newcommand{\subc}{{\sc SubChunk}\xspace}



%COMPRESS
\newcommand{\squeezeup}{\vspace{-2.5mm}}

% Debugging
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}
\newcommand{\amol}[1]{\textcolor{red}{[Amol: #1]}}
\newcommand{\resolved}[1]{\textcolor{green}{[Amol: #1]}}

\newcommand{\eat}[1]{}
\newcommand{\todo}[1]{\textcolor{blue}{[TODO: #1]}}


\title{
{\bf Thesis Proposal Title}\\
\vspace{18pt}
\it Preliminary Oral Exam (Thesis Proposal)}

\author{
{\bf Sudha Rao}  \\
Department of Computer Science \\
University of Maryland, College Park\\
{\texttt{raosudha@cs.umd.edu}}
}


\date{
\vspace{42pt}
Dissertation proposal submitted to: \\
Department of Computer Science \\
University of Maryland, College Park, MD 20742 \\
\bigskip
\bigskip
\today
\bigskip
\bigskip
\begin{table}[htp]
\begin{center}
\begin{tabular}{lll}
&\multicolumn{2}{l}{Advisory Committee:} \\ \\
Dr. Hal Daume III & Chair/Advisor & U. of Maryland, College Park \\
Dr. Philip Resnik & Advisor & U. of Maryland, College Park \\
Dr. David Jacobs & Dept's Rep & U. of Maryland, College Park 
\end{tabular}
\end{center}
\end{table}%
}


\begin{document}

\pagestyle{plain}
\pagenumbering{roman}

\maketitle
\pagebreak

\begin{abstract}
\normalsize

Asking questions is fundamental to communication, and machines cannot effectively collaborate with humans unless they can ask questions. Asking questions is also a natural way for machines to express uncertainty, a task of increasing importance in an automated society. Despite decades of work on question answering, there is relatively little work in question asking.

\end{abstract}

\pagebreak


\tableofcontents
\pagebreak

\cleardoublepage
\pagenumbering{arabic}


\chapter{Introduction}

\section{Motivation}
An overarching goal of the natural language processing community is to develop techniques that would enable machines to process naturally occurring text as efficiently as humans do. However, as humans, we do not always understand each other. According to Gricean pragmatics \cite{grice1975logic}, speakers and listeners adhere to a Cooperative Principle where a speaker communicates information that is as informative as required and not more. The extent of informativeness depends on the speaker's understanding of what she thinks is the common ground between the speaker and the listener. Communication is still possible because the listener makes use of this powerful tool of asking questions. By asking questions the listener can elicit information that she thinks is unclear or missing and thus attain a mutual understanding with the speaker. \cite{graesser2008question} identify that one of the key purposes of asking questions is the correction of such knowledge deficits.\\

\noindent
In the field of natural language processing, however, despite decades of work on question answering, there has been little work in question asking. Moreover most of the previous work on generating questions has been on generating reading comprehension style questions: given a text, write a question that one might find on a standardized test assessing the knowledge of a student about a particular topic in the text. For a machine to effectively collaborate with humans, it is important that it learns to ask a question when faced with uncertainty, a task of increasing importance in an automated society. For example, consider a scenario where an automated agent is trying to assist you in solving a problem. You will describe your problem to the agent. The agent on the other end has a certain understanding of what information is required to solve that problem. The agent can either make an assumption and work under uncertainty OR ask a question. The goal of this thesis work is to explore how can a machine automatically generate clarification questions when faced with such uncertainty or knowledge gaps

\section{Research questions}

About a decade ago, if you faced a technical problem the only way to solve it would be to go to an expert. Due to the recent surge in the use of internet, a lot of such problem solving happens online these days on question answering (Q\&A) forums where users post their problems and other users reply to them providing assistance. However \cite{asaduzzaman2013answering} observed that on StackExchange, one such community-driven problem solving platforms, many a times the posts go unanswered for a long time because they are not clear enough i.e. they are missing some information. Other users then ask clarification questions to those posts so that they can better offer assistance to the original poster. Our first research question is the following: \textit{Can we build a model that can learn to automatically generate a clarification question to a post by looking at clarification questions posted in the forum before?}\\

\noindent
Human-bot interaction has become increasingly popular in recent times. Apple's siri, Amazon's alexa, IBM's watson are all examples of successful advancements in the Artificial Intelligence technology. These technologies are great when it comes to simple natural language based interactions like ``What is the weather like in New York City?'' OR ``Play me latest top 10 songs'', etc. However when one tries to use these interfaces for more complicated purposes like ``I can't start my laptop. Help me\!'', OR ``Find me a recipe for lasagne'', within a few interactions one would soon give up. One key reasons for such failures is the lack of common understanding between the human user and the bot. The human user has a certain understanding of the problem/request he has and many a times he fails to convey the same understanding to the bot. In such a scenario, the bot can be much more useful if it could try to establish this common understanding by asking relevant questions. Our second research question is: \textit{Can we build an interactive model that can learn to ask clarification questions when faced with uncertainty or a knowledge gap?}\\

\noindent
% clarification question with the help of knowledge base i.e. find out what is missing and then ask a question

\section{Proposed solutions}

In order to learn how to ask clarification questions, we build a model inspired by the decision theoretic framework of expected value of perfect information (EVPI). EVPI is a measurement of of the value of gathering information. A good question is the one whose likely answer is going to be the most useful. In our setting, we use EVPI to calculate which question is most likely to elicit an answer that would make the post more informative. On StackExchange, users routinely ask clarification questions to post. The author of the post subsequently edits the post answering the question. We mine many (post, question, answer) triples using StackExchange's edit histories. We extract the initial post as p, question posted in the comments section as q, and edit to the original post as answer a to form our (p,q,a) triples. Using this data, we build our EVPI inspired neural network model which learns to ask a clarification question given a post.\\

\noindent
In our preliminary work, we focus on the question selection problem i.e. select the right clarification question from a set of prior questions. Given a post, we first generate a set of candidate questions by identifying posts similar to the given post and then looking at the questions asked to those posts. For identifying similar posts, we use Lucene \footnote{https://lucene.apache.org/}, a software extensively used in information retrieval for extracting documents relevant to a given query from a pool of documents. To enable our system to generalize better on new unseen cases, we propose two research directions: a template based question generation method where we first generate templates and then fill in the variables using the current context of the post; and a sequence-to-sequence based neural generative model where we generate the question one word at a time given a post.\\

\noindent
We study the problem of generation clarification questions in a dialog setting using the Ubuntu Dialog corpus, a large corpus recently put together by \cite{} by extracting two-way conversations from chat logs where people were discussing about the issues they were having with the Ubuntu Operating system. In our preliminary work, we consider the problem of next utterance classification i.e. given a context of a conversation and a set of possible next responses, choose the correct next response. %Recently, \cite{serban2016multiresolution} proposed a multi-resolution recurrent neural network model where they generate the high-level coarse tokens like nouns, entities and activities using a separate layer in their model thus indirectly giving more importance to the topic words in a conversation. Our model, on the other hand, that captures a similar notion using what we call the `bursty vocabulary' i.e. we up-weight the words that occur frequently within the short context of a conversation, thus avoiding the need for manually crafting the list of coarse tokens. 
We build a novel neural network based model using what we call the `bursty vocabulary' i.e. up-weighting the words that occur frequently within the short context of a conversation, thus indirectly giving more importance to topic words. This approach gives us significant improvements over current neural network baselines. In our next step, we propose to look at the responses in the conversations that are questions and build a model that learns to generate a question based response given different levels of context of a conversation.

%this para -- we also explore question generation in dialogue setting. work in dialogue -- next utterance classification -- moving on to generating questions in mult-turn dialogue -- generative models on ubuntu dialogue dataset

\newpage

\chapter{Related work}

\section{Question Generation}

The problem of question generation has received sparse attention from the natural language processing community. Most prior work focuses on generating reading comprehension questions:  given text, write questions that one might find on a standardized test \cite{vanderwende2008importance,heilman2011automatic,rus2011question,olney2012question}.  Comprehension questions, by definition, are answerable from the provided text. Clarification questions are not.  

Outside reading comprehension questions, \cite{labutov2015deep} generate high-level question templates by crowdsourcing and given a text segment, rank question templates that are relevant. However the crowdsourcing method of collecting data leads to significantly less data than we collect using our method. \cite{liu2010automatic} use template question generation to help authors write better related work sections. \cite{mostafazadeh2016generating} introduce a Visual Question Generation task where they consider question generation from images, a multi-modal variant of question generation. 
\cite{penas2010filling} identify the notion of missing information similar to us but they attempt to fill the knowledge gaps in a text with the help of external knowledge bases, whereas we instead ask clarification questions. \cite{artzi2011bootstrapping} use human-generated clarification questions to drive a semantic parser where the clarification questions are aimed towards simplifying a user query; whereas we generate clarification questions aimed at  identifying missing information in a text. 

- Question generation shared task: http://www.aclweb.org/anthology/W11-2853

\section{Neural networks in NLP}

\section{Dialogue modeling}

Following the introduction of the Ubuntu dialogue dataset, there has been several recent work in modeling dialogue that uses this dataset. \cite{DBLP:conf/sigdial/LowePSP15} describe two neural network baseline models using Recurrent Neural Network (RNN) and Long Short Term Memory (LSTM). 
\cite{DBLP:journals/corr/KadlecSK15} create an ensemble of LSTMs, Bi-LSTMs and CNNs to get improved baselines. 
\cite{xu2016incorporating} incorporate domain knowledge by enhancing their LSTM model with a recall gate.
\cite{serban2016hierarchical} introduce a latent variable hierarchical recurrent encoder-decoder model by adding a context level RNN that processes sequences of sub-sequences operating on top of the token level RNN.
\cite{serban2016multiresolution} take a similar hierarchical approach but instead of defining a latent coarse representation, they assume it to be observed and experiment with two such representations (sequence of nouns and sequence of activities and entities).
\cite{baudivs2016sentence} introduce a unified approach for scoring sentence pairs applicable to number of tasks including next utterance classification and show best result on the Ubuntu dialogue corpus task. 

The techniques we combine in this work are best practices gleaned from the NLP modeling literature. 
The character trigram histogram technique (Section~\ref{character_level_modeling}) was introduced as word hashing \cite{DBLP:conf/cikm/HuangHGDAH13} to reduce the dimensionality of bag-of-words term vectors. Residual learning (Section~\ref{residual_learning}) was first introduced as a way to ease training of very deep networks in image recognition work \cite{he2015deep}. The technique of attention aggregation (Section~\ref{attention_aggregation} has been effectively used for several language based tasks like neural machine translation \cite{DBLP:journals/corr/BahdanauCB14}, \cite{DBLP:conf/emnlp/LuongPM15}, question answering \cite{} and others. More specifically in dialogue modeling, \cite{DBLP:journals/corr/YaoPZW16} have shown that attending to the context while generating the words of the response, similar to attention based model used for MT, gives an improvement over using just the last hidden state.


\newpage

\chapter{Generating clarification questions on StackExchange posts}

\newpage

\chapter{Generating next response during two-way conversation}

\newpage

\chapter{Proposed Work}

\section{Template based question generation}

Our preliminary work focused on the question selection problem i.e. select the right clarification question from a set of prior questions. To enable our system to generalize better on new unseen cases, we propose a template based question generation method. Consider a template like ``What version of \_\_\_  are you running?''. This template can generate thousands of specific variants found in the data like ``What version of linux are you running?'',  ``What version of apt-get are you running?'', etc. We propose a following four-step approach:  (1) Cluster questions based on their similarity; (2) Generate a template for each cluster of questions; (3) Given a post, select a question template from a set of candidate templates; and finally (4) Fill in the blanks in the template using words relevant to the given post.

\section{Sequence-to-sequence based question generation}

Sequence-to-sequence neural network models have proven to be effective for several language generation tasks like machine translation (Sutskever et.al. 2014), dialog generation (Serban, Iulian V., et al. 2016), etc. On similar lines, we propose a model for generating a clarification question one word at a time, given the words of a post. A recent neural generative question answering model (Yin, Jun, et al.  2016) built an answer language model which decides, at each time step, whether to generate a common vocabulary word or a answer word retrieved from a knowledge base. Inspired from this work, we propose to build a question generation model which will decide, at each time step, whether to generate a common vocabulary word or a post specific word, thus incorporating the template based question generation method into a more general neural network framework.

\section{Clarification questions for dialog systems}

In our preliminary work we developed a model for generating a clarification question in a non-interactive setting i.e. after someone has described their problem in its entirety, we ask a question. However, most of such problem solving happens in the form of a dialog between a system and a human. In our next research direction, we propose to build a model that learns to generate  clarification questions at different stages of an ongoing conversation. Our model will learn the trade-off between asking a generic question at the initial stages and asking a specific question at the later stages of a conversation. We will evaluate our model using the Ubuntu Dialogue corpus, a large corpus recently put together by Lowe, Ryan, et al. (2015) by extracting two-way conversations from chat logs where people were discussing about the issues they were having with the Ubuntu Operating system.

\newpage

\begin{small}
\bibliographystyle{acl2017}
\bibliography{proposal}
\end{small}

\end{document}


